{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, we will compress Hamlet using arithmetic coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179096"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HAMLET = requests.get(\n",
    "    \"https://github.com/qi-rub/it-ss23-homework/raw/main/material/hamlet.txt\"\n",
    ").content.decode(\"ascii\", errors=\"ignore\")\n",
    "len(HAMLET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like last week, we use the following conventions:\n",
    "- **probability distributions** are represented by dictionaries that map symbols to probabilities, i.e., the probability $P(x)$ of outcome $x$ corresponds to the dictionary entry `P[x]`\n",
    "- **bitstrings** are represented by Python lists or tuples that contain 0s and 1s (for simplicity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last week, you implemented a function `empirical` that computed the empirical probability distribution of letters in a given string. Your solution probably looked similar to the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'H': 0.2, 'e': 0.2, 'l': 0.4, 'o': 0.2}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def empirical(s):\n",
    "    c = Counter(s)\n",
    "    N = len(s)\n",
    "    return {x: n / N for x, n in c.items()}\n",
    "\n",
    "\n",
    "empirical(\"Hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compression is intimely connected with modeling the source that we want to compress. We know this very clearly from Shannon's theorem, as well as from Huffman coding -- each relied explicitly on a probability distribution P modeling an IID data source.\n",
    "\n",
    "(Sometimes this also happens implicitly, such as in the LZ algorithm, where we can think of the phrases that are being collected during compression as a \"model\" of the data.)\n",
    "\n",
    "Arithmetic coding also relies on an explicit probabilistic model of the data. This week, we will consider so-called **digram models** (also known as bigram models). A diagram model is given by a **conditional probability distributions** $P(y|x)$, which models the probability that the next letter is $y$ if the previous letter was $x$.\n",
    "We will represent conditional probability distributions by Python dictionaries that map symbols to probability distributions, so that the conditional probability $P(y|x)$ is corresponds to the dictionary entry `P[x][y]`. Here, we also allow `x` to be `None` and interpret `P[None][y]` as the probability distribution of the first letter of the string.\n",
    "\n",
    "We can  we can use the `empirical` function from above to construct a very simple digram model, where `P[x][y]` equals by the frequency of `y` in the given string, so does not depend on `x` at all. In fact, this should better be called a \"unigram\" model, so that is the name that we use for the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{None: {'H': 0.2, 'e': 0.2, 'l': 0.4, 'o': 0.2},\n",
       " 'H': {'H': 0.2, 'e': 0.2, 'l': 0.4, 'o': 0.2},\n",
       " 'e': {'H': 0.2, 'e': 0.2, 'l': 0.4, 'o': 0.2},\n",
       " 'l': {'H': 0.2, 'e': 0.2, 'l': 0.4, 'o': 0.2},\n",
       " 'o': {'H': 0.2, 'e': 0.2, 'l': 0.4, 'o': 0.2}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unigram(s):\n",
    "    # compute frequencies of single letter\n",
    "    Q = empirical(s)\n",
    "\n",
    "    # use as P[x] for all letters, including the first (x = None)\n",
    "    P = {}\n",
    "    P[None] = Q\n",
    "    for x in s:\n",
    "        P[x] = Q\n",
    "    return P\n",
    "\n",
    "\n",
    "unigram(\"Hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly this model is silly, since it does not model any correlations between subsequent letters. **To do better, implement a function that computes the empirical conditional probability of subsequent letters in a given string.** That is, your function should compute\n",
    "$$P(y|x) = \\frac {N_{x,y}} {\\sum_{y'} N_{x,y'}},$$\n",
    "where $N_{x,y}$ denotes the number of times that `xy` appears as a substring of the given string.\n",
    "Please return the conditional distribution in the format described above, i.e., `P[x][y]` should correspond to $P(y|x)$.\n",
    "We already gave you a head start and took care of `P[None]` by setting it to \"deterministic\" probability distribution corresponding to the first letter of the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def digram(s):\n",
    "    P = {}\n",
    "\n",
    "    # the first letter is deterministic\n",
    "    P[None] = {s[0]: 1}\n",
    "\n",
    "    # TODO: can you implement the rest?\n",
    "\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code tests whether your function works correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = digram(\"Abracadabra\")\n",
    "\n",
    "# the first letter is always A\n",
    "assert P[None] == {\"A\": 1.0}\n",
    "\n",
    "# after A always follows b\n",
    "assert P[\"A\"] == {\"b\": 1.0}\n",
    "\n",
    "# after b always comes r\n",
    "assert P[\"b\"] == {\"r\": 1.0}\n",
    "\n",
    "# after r, c, d always comes a\n",
    "assert P[\"r\"] == {\"a\": 1.0} and P[\"c\"] == {\"a\": 1.0} and P[\"d\"] == {\"a\": 1.0}\n",
    "\n",
    "# after r comes either of {b, c, d}, with equal probabilities\n",
    "assert P[\"a\"] == {\"b\": 1 / 3, \"c\": 1 / 3, \"d\": 1 / 3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need two ingredients to build the arithmetic coding algorithm.\n",
    "The first is the ability to compute binary expansions of numbers $0 \\leq f < 1$\n",
    "Let $f=0.b_1b_2b_3\\dots$ be the standard binary expansion as discussed in class.\n",
    "**The following function should return the first `l` bits in the binary expansion of `f`. Can you implement it?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_expansion(f, l):\n",
    "    # TODO: can you implement this?\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some tests to make sure that your code works fine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert binary_expansion(1 / 3, 2) == [0, 1]\n",
    "assert binary_expansion(1 / 3, 10) == [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
    "assert binary_expansion(5 / 6, 2) == [1, 1]\n",
    "assert binary_expansion(5 / 6, 10) == [1, 1, 0, 1, 0, 1, 0, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second ingredient that we will need is the ability to compute lower and upper cumulative probabilities.\n",
    "We have implemented the first for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_cumulative(P):\n",
    "    symbols = sorted(P)\n",
    "    Q = {x: sum(P[y] for y in symbols[:k]) for k, x in enumerate(symbols)}\n",
    "    return Q\n",
    "\n",
    "\n",
    "P = {\"A\": 2 / 3, \"B\": 1 / 6, \"C\": 1 / 6}\n",
    "assert lower_cumulative(P) == {\"A\": 0, \"B\": 2 / 3, \"C\": 2 / 3 + 1 / 6}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Can you implement the following function to compute upper cumulative probabilities?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upper_cumulative(P):\n",
    "    # TODO: can you implement this?\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a test to make sure everything is in good order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = {\"A\": 2 / 3, \"B\": 1 / 6, \"C\": 1 / 6}\n",
    "assert upper_cumulative(P) == {\n",
    "    \"A\": 2 / 3,\n",
    "    \"B\": 2 / 3 + 1 / 6,\n",
    "    \"C\": 2 / 3 + 1 / 6 + 1 / 6,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Arithmetic Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to assemble all ingredients.\n",
    "**Your final task is to implement the arithmetic coding algorithm as discussed in class.** Your function should take as input a string of symbols (`s`) and a conditional probability distribution (`P`) corresponding the digram model, and it should return as output a bitstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arithmetic_encode(s, P):\n",
    "    # TODO: can you implement this?\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some tests of increasing difficulty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the probability distribution discussed in class (P(A) = 2/3, P(B) = 1/3)\n",
    "P = unigram(\"AAB\")\n",
    "\n",
    "# for a single letter, arithmetic coding is the same as the Shannon-Fano-Elias code, so we can compare with the example discussed in class\n",
    "assert arithmetic_encode(\"A\", P) == [0, 1]\n",
    "assert arithmetic_encode(\"B\", P) == [1, 1, 0]\n",
    "\n",
    "# now compress a string of length 300 (note that 277/300 is already close to the entropy)\n",
    "assert len(arithmetic_encode(\"AAB\" * 100, P)) == 277"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = \"Hello\"\n",
    "assert arithmetic_encode(TEXT, unigram(TEXT)) == [0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1]\n",
    "assert arithmetic_encode(TEXT, digram(TEXT)) == [0, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let us see how well we can compress Hamlet using either the language model.\n",
    "For the unigram model, your code should compare similarly to last week and reach about 58%.\n",
    "For the digram model, arithmetic coding should do markedly better -- does this match your output?\n",
    "\n",
    "*Hint: If your code runs too slow, try to avoid re-computing the cumultative probabilities from scratch for every input letter!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\"unigram\": unigram, \"digram\": digram}\n",
    "\n",
    "for name, model in models.items():\n",
    "    compressed = arithmetic_encode(HAMLET, model(HAMLET))\n",
    "    compressed_bytes = len(compressed) / 8\n",
    "    R = compressed_bytes / len(HAMLET)\n",
    "    print(f\"Compression rate using {name} model: {R:2.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge problems (purely optional):**\n",
    "* Program a decompressor! The teaching assistants will explain to you how to do so in the tutorials...\n",
    "* Turn your code into a \"true\" streaming algorithm. That is, make sure that your algorithm still works if the input parameter is an iterable rather than a string of symbols.\n",
    "* Implement a trigraph model and see if it performs even better.\n",
    "* In fact, why don't you allow for language models that arbitrarily depend on preceding characters. It may be useful to introduce a `LanguageModel` base class with an `update(x)` function that is called upon reading a letter `x`, as well as functions `P(y)` and `Q(y)` that compute the lower and upper conditional cumulative probability of `y`.\n",
    "* For a true compressor, you would also need to store the probability distribution as part of the compressed data. An alternative is to use a language model that learns the language \"on the fly\". One way to learn the unigram distribution is *Laplace's rule*, which you discussed in the exercise class. Implement it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
